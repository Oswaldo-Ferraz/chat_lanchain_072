"""
Chatbot RAG Otimizado - Sistema de Suporte T√©cnico para Britadeiras
Vers√£o: 2.0 - Otimizada com Cache, Async, Modelos H√≠bridos e Streaming
"""

# Imports
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import TextLoader
from langchain.globals import set_llm_cache
from langchain_community.cache import InMemoryCache
import yaml
import os
import asyncio
import time

def setup_sistema():
    """Configurar sistema com cache e modelos otimizados"""
    print("üöÄ Iniciando setup do Chatbot RAG Otimizado...")
    
    # Carregar configura√ß√µes
    with open("../config/config.yaml", "r") as config_file:
        config = yaml.safe_load(config_file)
    os.environ["OPENAI_API_KEY"] = config["OPENAI_API_KEY"]
    
    # Configurar cache para respostas mais r√°pidas
    set_llm_cache(InMemoryCache())
    
    # Modelos otimizados
    openai_rapido = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)    # Para an√°lises r√°pidas
    openai_premium = ChatOpenAI(model='gpt-4', temperature=0)          # Para resposta final
    
    print("‚úÖ Setup conclu√≠do com sucesso!")
    print("üî• Cache ativado para respostas mais r√°pidas!")
    print(f"‚ö° Modelo r√°pido: {openai_rapido.model_name}")
    print(f"üîß Modelo premium: {openai_premium.model_name}")
    print(f"üå°Ô∏è Temperatura: {openai_premium.temperature}")
    print("üöÄ Sistema otimizado pronto para uso!")
    
    return openai_rapido, openai_premium

def carregar_documentos():
    """Carregar base de conhecimento"""
    print("\nüìö Carregando base de conhecimento...")
    
    loader = TextLoader('../data/base_conhecimento_britadeira.txt')
    documents = loader.load()
    
    print("‚úÖ Documentos carregados:")
    for doc in documents:
        first_line = doc.page_content.split('\n')[0]
        print(f"- {first_line}")
    
    return documents

def preparar_inputs(documents, pergunta, historico_conversas=""):
    """Preparar e otimizar inputs do sistema"""
    print("\n‚öôÔ∏è Preparando inputs otimizados...")
    
    # Verificar se todas as vari√°veis est√£o presentes
    variaveis_necessarias = {
        'documents': documents is not None and len(documents) > 0,
        'pergunta': pergunta is not None and pergunta.strip() != "",
        'historico_conversas': True  # Opcional
    }
    
    # Verificar quais est√£o faltando
    faltando = [var for var, presente in variaveis_necessarias.items() if not presente]
    
    if not faltando:
        # Otimizar contexto para melhor performance
        context_completo = "\n".join(doc.page_content for doc in documents)
        context_otimizado = context_completo[:8000] if len(context_completo) > 8000 else context_completo
        
        inputs = {
            "context": context_otimizado,
            "question": pergunta,
            "historico": historico_conversas
        }
        
        print("‚úÖ Inputs preparados com sucesso!")
        if len(context_completo) > 8000:
            print("‚ö° Contexto otimizado para melhor performance!")
        
        return inputs
    else:
        print("‚ö†Ô∏è ERRO - Vari√°veis faltando:")
        for var in faltando:
            print(f"‚ùå {var} n√£o est√° presente ou est√° vazia")
        return None

def criar_prompts():
    """Criar templates de prompts especializados"""
    print("\nüéØ Criando prompts especializados...")
    
    prompt_base_conhecimento = PromptTemplate(
        input_variables=["context", "question"],
        template="""Use o seguinte contexto para responder √† pergunta. 
        Responda apenas com base nas informa√ß√µes fornecidas.
        N√£o fornece√ßa instru√ß√µes de procedimento j√° realizados.
        N√£o utilize informa√ß√µes externas ao contexto:
        Contexto: {context}
        Pergunta: {question}"""
    )
    
    prompt_historico_conversas = PromptTemplate(
        input_variables=["historico", "question"],
        template="""Use o hist√≥rico de conversas para responder √† pergunta. 
        Responda apenas com base nas informa√ß√µes fornecidas. 
        N√£o fornece√ßa instru√ß√µes de procedimento j√° realizados.
        N√£o utilize informa√ß√µes externas ao contexto:
        Hist√≥rico: {historico}
        Pergunta: {question}"""
    )
    
    prompt_final = PromptTemplate(
        input_variables=["resposta_base_conhecimento", "resposta_historico_conversas"],
        template="""Combine as seguintes respostas para gerar uma resposta final,
        mas n√£o forne√ßa instru√ß√µes de procedimentos j√° realizados:
        Resposta da base de conhecimento: {resposta_base_conhecimento}
        Resposta do hist√≥rico de conversas: {resposta_historico_conversas}"""
    )
    
    print("‚úÖ Prompts templates criados com sucesso!")
    return prompt_base_conhecimento, prompt_historico_conversas, prompt_final

def criar_chains(prompts, openai_rapido, openai_premium):
    """Criar cadeias otimizadas com modelos h√≠bridos"""
    print("\nüîó Criando cadeias otimizadas...")
    
    prompt_base_conhecimento, prompt_historico_conversas, prompt_final = prompts
    
    # Cadeias otimizadas com modelos h√≠bridos
    chain_base_conhecimento = prompt_base_conhecimento | openai_rapido      # GPT-3.5 para an√°lise r√°pida
    chain_historico_conversas = prompt_historico_conversas | openai_rapido  # GPT-3.5 para an√°lise r√°pida  
    chain_final = prompt_final | openai_premium                             # GPT-4 para resposta final
    
    print("‚úÖ Cadeias otimizadas definidas com sucesso!")
    print("‚ö° An√°lises iniciais: GPT-3.5 Turbo (r√°pido)")
    print("üéØ Resposta final: GPT-4 (premium)")
    
    return chain_base_conhecimento, chain_historico_conversas, chain_final

async def executar_chains_otimizada(chains, inputs):
    """Executar chains de forma ass√≠ncrona com medi√ß√£o de tempo"""
    chain_base_conhecimento, chain_historico_conversas, chain_final = chains
    
    inicio = time.time()
    print("\n‚è±Ô∏è Iniciando execu√ß√£o das chains otimizadas...")
    
    # Executar as duas primeiras chains em paralelo (GPT-3.5)
    inicio_paralelo = time.time()
    resultado_base, resultado_historico = await asyncio.gather(
        chain_base_conhecimento.ainvoke({"context": inputs["context"], "question": inputs["question"]}),
        chain_historico_conversas.ainvoke({"historico": inputs["historico"], "question": inputs["question"]})
    )
    fim_paralelo = time.time()
    print(f"‚ö° An√°lises paralelas (GPT-3.5) executadas em: {fim_paralelo - inicio_paralelo:.2f}s")
    
    # Executar chain final com GPT-4
    inicio_final = time.time()
    resultado_final = await chain_final.ainvoke({
        "resposta_base_conhecimento": resultado_base, 
        "resposta_historico_conversas": resultado_historico
    })
    fim_final = time.time()
    print(f"üéØ Resposta final (GPT-4) executada em: {fim_final - inicio_final:.2f}s")
    
    fim = time.time()
    print(f"üèÅ TEMPO TOTAL OTIMIZADO: {fim - inicio:.2f}s")
    
    return resultado_base, resultado_historico, resultado_final

async def resposta_streaming(chain_final, resultado_base, resultado_historico, pergunta):
    """Gerar resposta com streaming em tempo real"""
    print("\n" + "="*50)
    print("ü§ñ CHATBOT RAG OTIMIZADO - RESPOSTA EM TEMPO REAL")
    print("=" * 50)
    print(f"‚ùì Pergunta:\n{pergunta}")
    print("\n" + "=" * 50)
    print("üí¨ Resposta:")
    
    # Streaming da resposta final em tempo real
    resposta_completa = ""
    inicio_stream = time.time()
    
    async for chunk in chain_final.astream({
        "resposta_base_conhecimento": resultado_base,
        "resposta_historico_conversas": resultado_historico
    }):
        print(chunk.content, end="", flush=True)
        resposta_completa += chunk.content
    
    fim_stream = time.time()
    
    print(f"\n\n" + "=" * 50)
    print(f"‚ö° Streaming executado em: {fim_stream - inicio_stream:.2f}s")
    print("‚úÖ Resposta otimizada gerada com base na documenta√ß√£o\nt√©cnica e hist√≥rico de conversas")
    print("üî• Sistema utilizando: Cache + Async + Modelos H√≠bridos + Streaming")
    
    return resposta_completa

async def processar_pergunta(pergunta, historico_conversas=""):
    """Fun√ß√£o principal para processar uma pergunta"""
    print("="*70)
    print("ü§ñ CHATBOT RAG OTIMIZADO - INICIANDO PROCESSAMENTO")
    print("="*70)
    
    # 1. Setup do sistema
    openai_rapido, openai_premium = setup_sistema()
    
    # 2. Carregar documentos
    documents = carregar_documentos()
    
    # 3. Preparar inputs
    inputs = preparar_inputs(documents, pergunta, historico_conversas)
    if not inputs:
        return None
    
    # 4. Criar prompts
    prompts = criar_prompts()
    
    # 5. Criar chains
    chains = criar_chains(prompts, openai_rapido, openai_premium)
    
    # 6. Executar processamento
    resultado_base, resultado_historico, resultado_final = await executar_chains_otimizada(chains, inputs)
    
    # 7. Mostrar resultados detalhados (opcional)
    print("\n" + "="*60)
    print("üìä RESULTADOS DAS AN√ÅLISES:")
    print("="*60)
    print("üîç Resultado Base de Conhecimento:\n", resultado_base.content)
    print("\n" + "-"*40)
    print("üìù Resultado Hist√≥rico de Conversas:\n", resultado_historico.content)
    print("\n" + "-"*40)
    print("üéØ Resultado Final Combinado:\n", resultado_final.content)
    
    # 8. Resposta final com streaming
    resposta_final = await resposta_streaming(chains[2], resultado_base, resultado_historico, pergunta)
    
    return resposta_final

# Fun√ß√£o para uso direto
async def main():
    """Executar exemplo de uso"""
    # Dados de exemplo
    historico_conversas = """Cliente: Minha britadeira n√£o liga. Chatbot: Voc√™ j√° verificou 
                             se a bateria est√° carregada e conectada corretamente?"""
    
    pergunta = "Minha britadeira n√£o liga. Eu j√° verifiquei e a bateria est√° carregada e conectada corretamente"
    
    # Processar pergunta
    resposta = await processar_pergunta(pergunta, historico_conversas)
    
    return resposta

# Executar se for chamado diretamente
if __name__ == "__main__":
    # Para executar em ambiente ass√≠ncrono
    resposta = asyncio.run(main())
